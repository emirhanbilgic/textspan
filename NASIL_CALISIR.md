# ğŸ§  CLIP Heatmap Sistemi NasÄ±l Ã‡alÄ±ÅŸÄ±yor?

## ğŸ“š Genel BakÄ±ÅŸ

Bu sistem, CLIP (Contrastive Language-Image Pre-training) modelinin bir gÃ¶rÃ¼ntÃ¼yÃ¼ anlarken **nereye baktÄ±ÄŸÄ±nÄ±** gÃ¶steriyor. Her prompt iÃ§in modelin farklÄ± bÃ¶lgelere odaklandÄ±ÄŸÄ±nÄ± gÃ¶rebiliyoruz.

---

## ğŸ”¬ AdÄ±m AdÄ±m SÃ¼reÃ§

### 1ï¸âƒ£ **GÃ¶rÃ¼ntÃ¼yÃ¼ Model Ä°Ã§inden GeÃ§irme**

```python
representation = model.encode_image(image.to(device), 
                                  attn_method='head', 
                                  normalize=False)
attentions, mlps = prs.finalize(representation)
```

**Ne oluyor?**
- GÃ¶rÃ¼ntÃ¼ 224Ã—224 boyutuna resize ediliyor
- ViT (Vision Transformer) 16Ã—16 patch'lere bÃ¶lÃ¼yor = **256 patch**
- Model 24 layer'dan geÃ§iyor (ViT-L-14 iÃ§in)
- Her layer'da **16 attention head** var
- Her head, her patch'in diÄŸer patch'lerle nasÄ±l "iliÅŸkili" olduÄŸunu Ã¶ÄŸreniyor

**Attention Shape:**
```
attentions shape: [1, 24, 257, 16, 1024]
                   â”‚   â”‚   â”‚    â”‚   â””â”€ embedding dimension (1024)
                   â”‚   â”‚   â”‚    â””â”€â”€â”€â”€â”€ 16 attention heads
                   â”‚   â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 257 tokens (1 CLS + 256 patches)
                   â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 24 layers
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ batch size (1)
```

### 2ï¸âƒ£ **Text Prompt'larÄ± Encode Etme**

```python
texts = tokenizer(prompts).to(device)
class_embeddings = model.encode_text(texts)
class_embedding = F.normalize(class_embeddings, dim=-1)
```

**Ne oluyor?**
- Her prompt (Ã¶rn: "a photo of a dog") CLIP'in text encoder'Ä±ndan geÃ§iyor
- Her prompt bir **1024-boyutlu vektÃ¶r** haline geliyor
- 5 prompt = 5 vektÃ¶r

**Ã‡Ä±kan Shape:**
```
class_embedding shape: [5, 1024]
                        â”‚   â””â”€ embedding dimension
                        â””â”€â”€â”€â”€â”€ 5 different prompts
```

### 3ï¸âƒ£ **EN Ã–NEMLÄ° ADIM: Attention Ã— Text BenzerliÄŸi**

```python
attention_map = attentions[0, :, 1:, :].sum(axis=(0,2)) @ class_embedding.T
```

Bu satÄ±r Ã‡OK Ã–NEMLÄ! ParÃ§alayalÄ±m:

#### ğŸ”¹ **AdÄ±m A:** `attentions[0, :, 1:, :]`
- `[0]` â†’ batch'den 1. gÃ¶rÃ¼ntÃ¼
- `[:, 1:, :]` â†’ CLS token'Ä± atla (index 0), sadece 256 image patch'ini al
- Shape: `[24, 256, 16, 1024]`

#### ğŸ”¹ **AdÄ±m B:** `.sum(axis=(0,2))`
- `axis=0` â†’ 24 layer'Ä± topla (tÃ¼m layer'larÄ±n katkÄ±sÄ±)
- `axis=2` â†’ 16 attention head'i topla (tÃ¼m head'lerin katkÄ±sÄ±)
- Shape: `[256, 1024]`
- Yani: Her patch iÃ§in 1024-boyutlu bir "Ã¶zet vektÃ¶r"

#### ğŸ”¹ **AdÄ±m C:** `@ class_embedding.T`
- Matrix Ã§arpÄ±mÄ± (dot product)
- `[256, 1024] @ [1024, 5]` = `[256, 5]`
- Her patch iÃ§in her prompt'la **benzerlik skoru** hesaplanÄ±yor!

**SonuÃ§:** 256 patch Ã— 5 prompt = Her patch'in her prompt'la ne kadar "uyumlu" olduÄŸu

### 4ï¸âƒ£ **Spatial Heatmap'e DÃ¶nÃ¼ÅŸtÃ¼rme**

```python
attention_map = F.interpolate(
    einops.rearrange(attention_map, '(B N M) C -> B C N M', N=16, M=16, B=1), 
    scale_factor=model.visual.patch_size[0],
    mode='bilinear'
).to(device)
```

**Ne oluyor?**
1. `[256, 5]` â†’ `[1, 5, 16, 16]` reshape (16Ã—16 grid'e dÃ¶nÃ¼ÅŸÃ¼yor)
2. `scale_factor=14` ile 224Ã—224'e upscale (bilinear interpolation)
3. Son shape: `[1, 5, 224, 224]`

### 5ï¸âƒ£ **Normalizasyon ve GÃ¶rselleÅŸtirme**

```python
v = attention_map[idx] - np.mean(attention_map, axis=0)
```

**Ã‡OK Ã–NEMLÄ:** Bu satÄ±r **"relative attention"** hesaplÄ±yor!

- Her prompt'un attention'Ä±, **ortalamaya gÃ¶re normalize ediliyor**
- Yani: "Bu prompt iÃ§in model ORTALAMAYA GÃ–RE NE KADAR FAZLA/AZ bakÄ±yor?"
- Pozitif deÄŸer â†’ Model bu prompt iÃ§in bu bÃ¶lgeye FAZLA bakÄ±yor â†’ KIRMIZI
- Negatif deÄŸer â†’ Model bu prompt iÃ§in bu bÃ¶lgeye AZ bakÄ±yor â†’ MAVÄ

```python
v_normalized = (v - v_min) / (v_max - v_min)
v_uint8 = np.uint8(v_normalized * 255)
heatmap_colored = cv2.applyColorMap(v_uint8, cv2.COLORMAP_JET)
```

- Min-max normalizasyonu â†’ [0, 1] aralÄ±ÄŸÄ±na
- 0-255 aralÄ±ÄŸÄ±na Ã§evir
- JET colormap uygula (MAVÄ° â†’ KIRMIZI spektrumu)

---

## ğŸ¤” Prompt'lar Birbirini Etkiliyor mu?

### âœ… **EVET ve HAYIR!**

#### 1ï¸âƒ£ **Model Ã‡alÄ±ÅŸtÄ±rma AÅŸamasÄ±nda: HAYIR**
```python
# Her prompt AYRI AYRI encode ediliyor, birbirini ETKÄ°LEMÄ°YOR
for each_prompt in prompts:
    encoding = model.encode_text(each_prompt)  # BaÄŸÄ±msÄ±z!
```

Her prompt baÄŸÄ±msÄ±z olarak encode ediliyor. Model bir prompt'u iÅŸlerken diÄŸerlerinden habersiz.

#### 2ï¸âƒ£ **GÃ¶rselleÅŸtirme AÅŸamasÄ±nda: EVET**
```python
v = attention_map[idx] - np.mean(attention_map, axis=0)
#                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
#                         TÃœM prompt'larÄ±n ortalamasÄ±!
```

Heatmap'i oluÅŸtururken **tÃ¼m prompt'larÄ±n ortalamasÄ±na gÃ¶re** normalize ediyoruz!

**Ã–rnek:**
- EÄŸer gÃ¶rÃ¼ntÃ¼de bir kÃ¶pek varsa:
  - "a photo of a dog" â†’ KÃ¶pek bÃ¶lgesi YÃœKSEK skor (ortalamadan FAZLA)
  - "a photo of a car" â†’ KÃ¶pek bÃ¶lgesi DÃœÅÃœK skor (ortalamadan AZ)

Bu yÃ¼zden:
- ğŸ”´ **KIRMIZI** = Model bu prompt iÃ§in bu bÃ¶lgeye ODAKLANMIÅ
- ğŸ”µ **MAVÄ** = Model bu prompt iÃ§in bu bÃ¶lgeyi GÃ–RMEZDEN GELMÄÅ

---

## ğŸ¯ Matematiksel Ã–zet

1. **Image Encoding:** `I â†’ ViT â†’ attentions[24, 256, 16, 1024]`
2. **Text Encoding:** `T â†’ TextEncoder â†’ embeddings[5, 1024]`
3. **Aggregation:** `attentions.sum(layers, heads) â†’ [256, 1024]`
4. **Similarity:** `[256, 1024] @ [1024, 5] â†’ [256, 5]`
5. **Reshape:** `[256, 5] â†’ [16, 16, 5] â†’ [224, 224, 5]`
6. **Absolute Score:** `score[i] â†’ absolute_attention` (her prompt baÄŸÄ±msÄ±z)
7. **Visualization:** `absolute_attention â†’ COLORMAP â†’ HEATMAP`

---

## ğŸ” Neden Bu Kadar Ä°yi Ã‡alÄ±ÅŸÄ±yor?

### CLIP'in GÃ¼cÃ¼:
1. **400 milyon** gÃ¶rÃ¼ntÃ¼-text Ã§ifti ile eÄŸitilmiÅŸ
2. **Contrastive Learning:** DoÄŸru gÃ¶rÃ¼ntÃ¼-text Ã§iftleri yakÄ±n, yanlÄ±ÅŸlar uzak
3. **Vision Transformer:** Her patch diÄŸer patch'lerle "konuÅŸuyor" (attention)
4. **Multi-Head Attention:** 16 farklÄ± bakÄ±ÅŸ aÃ§Ä±sÄ± aynÄ± anda

### PRS (Projected Residual Stream) Metodu:
Bu projede kullanÄ±lan Ã¶zel teknik:
- Her layer'Ä±n katkÄ±sÄ±nÄ± ayrÄ± ayrÄ± izliyor
- Her attention head'in ne yaptÄ±ÄŸÄ±nÄ± gÃ¶rebiliyoruz
- Bu sayede "model nereye bakÄ±yor?" sorusunu cevaplayabiliyoruz

---

## ğŸ’¡ SonuÃ§

**Heatmap'ler ÅŸunu gÃ¶steriyor:**
> Model bir gÃ¶rÃ¼ntÃ¼yÃ¼ gÃ¶rÃ¼nce, verilen prompt'a gÃ¶re gÃ¶rÃ¼ntÃ¼nÃ¼n **hangi bÃ¶lgelerinin o prompt'la uyumlu olduÄŸunu** hesaplÄ±yor.

- "a photo of a dog" â†’ KÃ¶pek olan bÃ¶lgeler aktivasyonu artÄ±yor â†’ KIRMIZI
- "a photo of a car" â†’ Araba olan bÃ¶lgeler aktivasyonu artÄ±yor â†’ KIRMIZI

Her prompt iÃ§in model **baÄŸÄ±msÄ±z bir karar veriyor**, ama gÃ¶rselleÅŸtirmede hepsini **karÅŸÄ±laÅŸtÄ±rmalÄ± olarak** gÃ¶steriyoruz!

---

## ğŸ“– Kaynaklar

Bu proje ÅŸu makaleye dayanÄ±yor:
- **Paper:** "Interpreting CLIP's Image Representation via Text-Based Decomposition"
- **Authors:** Yossi Gandelsman, Alexei A. Efros, Jacob Steinhardt
- **Conference:** ICLR 2024

